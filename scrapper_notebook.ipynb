{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize all the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "from typing import Dict, List, Tuple\n",
    "from datetime import datetime\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import Graph, StateGraph\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from typing import Optional, List\n",
    "from newspaper import Article\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.common.by import By\n",
    "import html2text\n",
    "import csv\n",
    "from dateutil import parser\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class WebScraper:\n",
    "    def __init__(self):\n",
    "        self.bedrock = boto3.client(\n",
    "            service_name='bedrock-runtime',\n",
    "            region_name='us-west-2'\n",
    "        )\n",
    "\n",
    "    def _get_html_newspaper(self, url: str) -> Optional[str]:\n",
    "        \"\"\"Attempt to get HTML content using newspaper3k.\"\"\"\n",
    "        try:\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            return article.html\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    def _get_html_selenium(self, url: str) -> Optional[str]:\n",
    "        \"\"\"Get HTML content using Selenium as fallback.\"\"\"\n",
    "        try:\n",
    "            chrome_options = Options()\n",
    "            chrome_options.add_argument('--headless')\n",
    "            chrome_options.add_argument('--disable-gpu')\n",
    "            chrome_options.add_argument('--no-sandbox')\n",
    "            \n",
    "            driver = webdriver.Chrome(options=chrome_options)\n",
    "            driver.get(url)\n",
    "            text_content = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "            driver.quit()\n",
    "            return text_content\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    def get_page_html(self, url: str) -> Optional[str]:\n",
    "        \"\"\"Get HTML content using newspaper3k first, then selenium as fallback.\"\"\"\n",
    "        html = self._get_html_newspaper(url)\n",
    "        if not html:\n",
    "            html = self._get_html_selenium(url)\n",
    "        return html\n",
    "\n",
    "    def convert_html_to_markdown(self, html: str) -> Optional[str]:\n",
    "        \"\"\"Convert HTML content to markdown format.\"\"\"\n",
    "        try:\n",
    "            # Using html2text for conversion\n",
    "            h = html2text.HTML2Text()\n",
    "            h.ignore_links = True\n",
    "            h.ignore_images = True\n",
    "            h.ignore_tables = True\n",
    "            h.body_width = 0  # Don't wrap text\n",
    "            markdown = h.handle(html)\n",
    "            return markdown.strip()\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    def fetch_markdown(self, url: str) -> str:\n",
    "        \"\"\"Fetch markdown content using Jina.\"\"\"\n",
    "        html = self.get_page_html(url)\n",
    "        return self.convert_html_to_markdown(html)\n",
    "\n",
    "    def invoke_bedrock(self, prompt: str) -> str:\n",
    "        \"\"\"Invoke Bedrock model with the given prompt.\"\"\"\n",
    "\n",
    "        # save promt in txt file\n",
    "        with open(\"prompt.txt\", \"w\") as f:\n",
    "            f.write(prompt)\n",
    "\n",
    "        body = json.dumps({\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_p\": 1,\n",
    "            \"max_gen_len\": 8192,\n",
    "            \"stop\": []\n",
    "        })\n",
    "\n",
    "        response = self.bedrock.invoke_model(\n",
    "            modelId=\"us.meta.llama3-3-70b-instruct-v1:0\",\n",
    "            body=body\n",
    "        )\n",
    "        \n",
    "        # Debug: read streaming body content\n",
    "        raw_response = response['body'].read().decode('utf-8')\n",
    "        \n",
    "        try:\n",
    "            response_body = json.loads(raw_response)\n",
    "            # Debug: print parsed JSON structure\n",
    "            print(\"Response structure:\", json.dumps(response_body, indent=2))\n",
    "            return response_body.get('generation', '').strip()\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing response: {e}\")\n",
    "            return raw_response.strip()\n",
    "\n",
    "    def extract_content(self, markdown: str, url: str) -> Dict:\n",
    "        \"\"\"Extract only the publication date from markdown.\"\"\"\n",
    "        prompt = f\"\"\"You are an assistant that finds publication dates in text.\n",
    "\n",
    "Look at the text below and find the publication date of the article:\n",
    "\n",
    "{markdown}\n",
    "\n",
    "The URL of the article is: {url}   \n",
    "\n",
    "Return ONLY the publication date in YYYY-MM-DD format. If the date is incomplete, use X for unknown parts (like XXXX-XX-XX).\n",
    "\n",
    "Respond ONLY with the date in YYYY-MM-DD format. No other text or explanation.\n",
    "\n",
    "If you cannot find the publication date in the article's metadata, content, or timestamps, try to extract it from the URL pattern. Many news sites include dates in their URL structures (like example.com/2023/05/12/article-title or example.com/news/article-title-20230512). Look for patterns that might represent a date (YYYY/MM/DD, YYYYMMDD, etc.) and extract this information as a potential publication date, but indicate that it was derived from the URL rather than an explicit publication date.\n",
    "\n",
    "If the page shows a '404 Not Found' error, indicates 'No Content Found', 'Page Content Not Found', or similar error messages, or if the page redirects to a press release page or homepage, do not attempt to extract a publication date. In these cases, return 'XXXX-XX-XX'.\n",
    "\n",
    "If you cannot find a publication or update date in the available text with reasonable confidence, or if the page has no publish date, return 'XXXX-XX-XX' rather than making a guess.\n",
    "\n",
    "If multiple dates are found in the content, return the most recent/latest date only.\n",
    "\n",
    "If a page seems to have a url not related to the content  like homepage , /press-release etc., return 'XXXX-XX-XX'\n",
    "\"\"\"\n",
    "\n",
    "        result = self.invoke_bedrock(prompt)\n",
    "        \n",
    "        # Clean up the result to ensure we just get the date\n",
    "        # Extract YYYY-MM-DD pattern from the response\n",
    "        date_match = re.search(r'(\\d{4}-\\d{2}-\\d{2})', result)\n",
    "        if date_match:\n",
    "            date_result = date_match.group(1)\n",
    "        else:\n",
    "            # Check for XXX pattern if no standard date is found\n",
    "            x_date_match = re.search(r'(X{4}-X{2}-X{2})', result)\n",
    "            if x_date_match:\n",
    "                date_result = x_date_match.group(1)\n",
    "            else:\n",
    "                date_result = \"XXXX-XX-XX\"\n",
    "            \n",
    "        return {\n",
    "            \"publish_date\": date_result\n",
    "        }\n",
    "\n",
    "def create_scraping_graph():\n",
    "    scraper = WebScraper()\n",
    "\n",
    "    # Define the nodes\n",
    "    def fetch_node(state):\n",
    "        url = state[\"url\"]\n",
    "        markdown = scraper.fetch_markdown(url)\n",
    "        state[\"markdown\"] = markdown\n",
    "        state[\"url\"] = url\n",
    "        return state\n",
    "\n",
    "    def extract_node(state):\n",
    "        markdown = state[\"markdown\"]\n",
    "        url = state[\"url\"]\n",
    "        content = scraper.extract_content(markdown, url)\n",
    "        state[\"result\"] = content\n",
    "        return state\n",
    "\n",
    "    # Create the graph\n",
    "    workflow = StateGraph(Graph())\n",
    "\n",
    "    # Add nodes\n",
    "    workflow.add_node(\"fetch\", fetch_node)\n",
    "    workflow.add_node(\"extract\", extract_node)\n",
    "\n",
    "    # Define edges\n",
    "    workflow.add_edge(\"fetch\", \"extract\")\n",
    "    workflow.set_entry_point(\"fetch\")\n",
    "\n",
    "    # Compile the graph\n",
    "    chain = workflow.compile()\n",
    "    return chain\n",
    "\n",
    "def scrape_website(url: str) -> Dict:\n",
    "    \"\"\"Main function to scrape and analyze a website.\"\"\"\n",
    "    chain = create_scraping_graph()\n",
    "    \n",
    "    # Initialize state\n",
    "    initial_state = {\n",
    "        \"url\": url\n",
    "    }\n",
    "    \n",
    "    # Execute the graph\n",
    "    result = chain.invoke(initial_state)\n",
    "    return result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Read URLs and expected dates from input_links.txt\n",
    "    input_data = []\n",
    "    with open(\"input_links.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                if len(parts) >= 2:\n",
    "                    url = parts[0].strip()\n",
    "                    expected_date = parts[1].strip()\n",
    "                    input_data.append((url, expected_date))\n",
    "    \n",
    "    # Create or open CSV file for writing results\n",
    "    with open(\"scraping_results.csv\", \"w\", newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        # Write header\n",
    "        csv_writer.writerow([\"URL\", \"Expected Date\", \"Scraped Date\"])\n",
    "        \n",
    "        # Process each URL\n",
    "        for url, expected_date in tqdm.tqdm(input_data, desc=\"Extracting publication dates\", unit=\"url\"):\n",
    "            print(f\"Processing URL: {url}\")\n",
    "            try:\n",
    "                result = scrape_website(url)\n",
    "                scraped_date = result.get(\"publish_date\", \"XXXX-XX-XX\")\n",
    "                # Write results to CSV\n",
    "                csv_writer.writerow([url, expected_date, scraped_date])\n",
    "                \n",
    "                # Also output to console\n",
    "                print(f\"URL: {url}\")\n",
    "                print(f\"Expected Date: {expected_date}\")\n",
    "                print(f\"Scraped Date: {scraped_date}\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {url}: {str(e)}\")\n",
    "                # Write error to CSV\n",
    "                csv_writer.writerow([url, expected_date, \"ERROR\"])\n",
    "            \n",
    "    print(f\"Results have been saved to scraping_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
